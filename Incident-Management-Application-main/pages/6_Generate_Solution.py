# import streamlit as st
# import torch
# import joblib
# from transformers import BartTokenizer, BartForConditionalGeneration
# from streamlit_feedback import streamlit_feedback

# # Load tokenizer and model
# @st.cache_resource
# def load_models():
#     tokenizer = BartTokenizer.from_pretrained("facebook/bart-base")
#     model = BartForConditionalGeneration.from_pretrained("facebook/bart-base")
#     model.load_state_dict(torch.load("/Users/hardikchhipa/Desktop/Data_Manipulations_Projects/Incident Autodesk/models/bart_model.pth", map_location=torch.device("cpu")))
#     model.eval()
    
#     resolution_time_model = joblib.load('/Users/hardikchhipa/Desktop/Data_Manipulations_Projects/Incident Autodesk/models/resolution_time_model.pkl')
#     vectorizer = joblib.load('/Users/hardikchhipa/Desktop/Data_Manipulations_Projects/Incident Autodesk/models/tfidf_vectorizer.pkl')
    
#     return tokenizer, model, resolution_time_model, vectorizer

# tokenizer, model, resolution_time_model, vectorizer = load_models()

# def generate_close_note(description):
#     inputs = tokenizer(description, return_tensors="pt", padding="max_length", truncation=True, max_length=128)
#     output_ids = model.generate(**inputs, max_length=128, num_beams=5, early_stopping=True)
#     close_note = tokenizer.decode(output_ids[0], skip_special_tokens=True)
#     return close_note

# # Streamlit UI
# st.title("üí¨  Jace")
# st.write("Chat with our AI to get estimated resolution time and possible solutions for your issue.")

# if "messages" not in st.session_state:
#     st.session_state.messages = [
#         {"role": "assistant", "content": "Hello! Describe your issue, and I'll estimate the resolution time and suggest a possible solution."}
#     ]
# if "response" not in st.session_state:
#     st.session_state["response"] = None

# messages = st.session_state.messages
# for msg in messages:
#     st.chat_message(msg["role"]).write(msg["content"])

# if prompt := st.chat_input(placeholder="Describe your issue here..."):
#     messages.append({"role": "user", "content": prompt})
#     st.chat_message("user").write(prompt)
    
#     # Transform input text using the loaded TF-IDF vectorizer
#     X_input = vectorizer.transform([prompt]).toarray()
    
#     # Predict resolution time
#     resolution_time = resolution_time_model.predict(X_input)[0]
    
#     # Generate possible close note
#     close_note = generate_close_note(prompt)
    
#     # Generate assistant response
#     response = f"**Estimated Resolution Time:** {resolution_time:.2f} days\n\n**Suggested Solution:** {close_note}"
#     st.session_state["response"] = response
    
#     with st.chat_message("assistant"):
#         messages.append({"role": "assistant", "content": response})
#         st.write(response)

# if st.session_state["response"]:
#     feedback = streamlit_feedback(
#         feedback_type="thumbs",
#         optional_text_label="[Optional] Please provide an explanation",
#         key=f"feedback_{len(messages)}",
#     )
    
#     if feedback:
#         st.toast("Feedback recorded!", icon="üìù")


import streamlit as st
import torch
import joblib
import gdown
import os
from transformers import BartTokenizer, BartForConditionalGeneration
from streamlit_feedback import streamlit_feedback


st.set_page_config(layout="wide")


# Custom CSS Styling
st.markdown("""
    <style>
    @import url('https://fonts.googleapis.com/css2?family=Space+Grotesk:wght@400;600&display=swap');
    
    * {
        font-family: 'Space Grotesk', sans-serif;
    }
    
    .main {
        background: #0f172a;
        color: #ffffff;
    }
    
    .header-card {
        background: linear-gradient(135deg, #1e3a8a 0%, #0f766e 100%);
        border-radius: 15px;
        padding: 2rem;
        margin: 1rem 0;
        box-shadow: 0 4px 30px rgba(0,0,0,0.1);
    }
    
    .chat-bubble {
        background: rgba(30, 41, 59, 0.8);
        border-radius: 15px;
        padding: 1.5rem;
        margin: 1rem 0;
        border: 1px solid #334155;
    }
    
    .user-message {
        border: 2px solid #3b82f6;
    }
    
    .assistant-message {
        border: 2px solid #10b981;
    }
    
    .stChatInput input {
        background: rgba(30, 41, 59, 0.8) !important;
        color: white !important;
        border-radius: 12px !important;
    }
    
    .metric-box {
        background: rgba(99, 102, 241, 0.15);
        border-radius: 12px;
        padding: 1rem;
        margin: 0.5rem;
        text-align: center;
    }
    </style>
""", unsafe_allow_html=True)

# Load tokenizer and model
@st.cache_resource
# def load_models():
#     tokenizer = BartTokenizer.from_pretrained("facebook/bart-base")
#     model = BartForConditionalGeneration.from_pretrained("facebook/bart-base")
#     model.load_state_dict(torch.load("/Users/hardikchhipa/Desktop/Data_Manipulations_Projects/Incident Autodesk/models/bart_model.pth", map_location=torch.device("cpu")))
#     model.eval()
    
#     resolution_time_model = joblib.load('/Users/hardikchhipa/Desktop/Data_Manipulations_Projects/Incident Autodesk/models/resolution_time_model.pkl')
#     vectorizer = joblib.load('/Users/hardikchhipa/Desktop/Data_Manipulations_Projects/Incident Autodesk/models/tfidf_vectorizer.pkl')
    
#     return tokenizer, model, resolution_time_model, vectorizer

# tokenizer, model, resolution_time_model, vectorizer = load_models()

def load_models():
    # Google Drive File IDs
    file_ids = {
        "bart_model": "13MZvpd4xo27Wi4BSUlJSLYz_E1sQgNSf",
        "resolution_model": "1xbFHgxHqRwGvez4xLz4qCt2g43xjDCXK",
        "vectorizer": "19ZQFmhlrjQNQ-y7xPz0jpMtNlolqSTEp",
    }
    
    # Temporary paths
    model_paths = {
        "bart_model": "bart_model.pth",
        "resolution_model": "resolution_time_model.pkl",
        "vectorizer": "tfidf_vectorizer.pkl",
    }
    
    # Download each file
    for key, file_id in file_ids.items():
        if not os.path.exists(model_paths[key]):
            gdown.download(f"https://drive.google.com/uc?id={file_id}", model_paths[key], quiet=False)

    # Load Models
    tokenizer = BartTokenizer.from_pretrained("facebook/bart-base")
    model = BartForConditionalGeneration.from_pretrained("facebook/bart-base")
    model.load_state_dict(torch.load(model_paths["bart_model"], map_location=torch.device("cpu")))
    model.eval()
    
    resolution_time_model = joblib.load(model_paths["resolution_model"])
    vectorizer = joblib.load(model_paths["vectorizer"])
    
    return tokenizer, model, resolution_time_model, vectorizer

# Load models
tokenizer, model, resolution_time_model, vectorizer = load_models()


def generate_close_note(description):
    inputs = tokenizer(description, return_tensors="pt", padding="max_length", truncation=True, max_length=128)
    output_ids = model.generate(**inputs, max_length=128, num_beams=5, early_stopping=True)
    close_note = tokenizer.decode(output_ids[0], skip_special_tokens=True)
    return close_note

# Page Header
st.markdown("""
    <div class="header-card">
        <h1 style='color: white; margin:0'>‚ú® Jace - AI Support Assistant</h1>
        <p style='color: #94a3b8; margin:0'>Get instant resolution estimates and technical solutions</p>
    </div>
""", unsafe_allow_html=True)


# Chat Interface
if "messages" not in st.session_state:
    st.session_state.messages = [
        {"role": "assistant", "content": "Hello! I'm Jace, your technical support AI. Describe your issue, and I'll provide:\n\n‚Ä¢ ‚è≥ Estimated resolution time\n‚Ä¢ üõ†Ô∏è Step-by-step solution\n‚Ä¢ üìö Relevant documentation links"}
    ]
if "response" not in st.session_state:
    st.session_state["response"] = None

messages = st.session_state.messages

for msg in messages:
    with st.chat_message(msg["role"]):
        with st.markdown(f'<div class="chat-bubble {msg["role"] + "-message"}>{msg["content"]}</div>', unsafe_allow_html=True):
            st.write(msg["content"])

if prompt := st.chat_input(placeholder="Describe your technical issue..."):
    messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        with st.markdown(f'<div class="chat-bubble user-message">{prompt}</div>', unsafe_allow_html=True):
            st.write(prompt)
    
    # Show loading animation
    with st.spinner("üîç Analyzing issue and formulating solution..."):
        # Transform input text
        X_input = vectorizer.transform([prompt]).toarray()
        
        # Predict resolution time
        resolution_time = resolution_time_model.predict(X_input)[0]
        
        # Generate possible close note
        close_note = generate_close_note(prompt)
    
    # Format response
    response = f"""
    <div style='padding: 1rem;'>
        <div style='color: #3b82f6; font-size:1.2rem; margin-bottom:1rem;'>‚è≥ Estimated Resolution Time: <strong>{resolution_time:.2f} days</strong></div>
        <div style='color: #10b981; font-size:1.2rem; margin-bottom:1rem;'>üõ†Ô∏è Suggested Solution:</div>
        <div style='background: rgba(16, 185, 129, 0.1); padding:1rem; border-radius:8px;'>
            {close_note}
        </div>
        <div style='margin-top:1rem; color: #94a3b8;'>
            üîç This solution was generated using our AI model trained on 45k+ support tickets
        </div>
    </div>
    """
    
    st.session_state["response"] = response
    
    with st.chat_message("assistant"):
        with st.markdown(f'<div class="chat-bubble assistant-message">{response}</div>', unsafe_allow_html=True):
            st.markdown(response, unsafe_allow_html=True)

if st.session_state["response"]:
    feedback = streamlit_feedback(
        feedback_type="thumbs",
        optional_text_label="Help us improve (optional):",
        align="flex-end",
        key=f"feedback_{len(messages)}",
    )
    
    if feedback:
        st.balloons()
        st.toast("üìù Thank you for your feedback! We'll use this to improve Jace.", icon="‚úÖ")

st.write("")
st.write("")
st.write("")
st.write("")
st.write("")
st.write("")
st.write("") 
st.write("")
st.write("")
st.write("")
st.write("")
st.write("")
st.write("")
st.write("")

# Stats Cards
col1, col2, col3 = st.columns(3)
with col1:
    st.markdown("""
        <div class="metric-box">
            <div style='color: #94a3b8'>üöÄ Avg Response Time</div>
            <div style='color: #3b82f6; font-size:1.8rem'><2s</div>
        </div>
    """, unsafe_allow_html=True)

with col2:
    st.markdown("""
        <div class="metric-box">
            <div style='color: #94a3b8'>üí° Solution Accuracy</div>
            <div style='color: #10b981; font-size:1.8rem'>93.7%</div>
        </div>
    """, unsafe_allow_html=True)

with col3:
    st.markdown("""
        <div class="metric-box">
            <div style='color: #94a3b8'>üìö Knowledge Base</div>
            <div style='color: #8b5cf6; font-size:1.8rem'>45K+</div>
        </div>
    """, unsafe_allow_html=True)


st.write("")
st.write("")
st.write("")
st.write("")
st.write("")
st.write("")
st.write("")

# Footer with Developer Credit
st.markdown("---")
st.markdown("""
    <div style='text-align:center; color:#64748b; margin: 2rem 0'>
        üöÄ Developed By <a href='https://www.linkedin.com/in/hardik-chhipa-303040242/' target='_blank' 
        style='color:#64748b; text-decoration:none; font-weight:bold;'>Hardik Chhipa</a>
    </div>
""", unsafe_allow_html=True)
